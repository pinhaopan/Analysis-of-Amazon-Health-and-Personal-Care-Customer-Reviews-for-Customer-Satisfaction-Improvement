{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1883a6be-de9b-4b64-9bc1-1918cd0964f5",
   "metadata": {},
   "source": [
    "#### This notebook is a record of our topic modeling process. In this notebook, we tried different topic modeling methods, including NMF, LDA, and BERTopic. Since topic modeling requires a lot of hyper-tuning, we kept a 2% sample of our merged dataset to ensure each run of hyper-tuning is within an acceptable time range. In our final notebook, we will use more data to ensure a more accurate and reliable analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "def105bd-a0ac-430c-822c-32c26e45da21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?export=download&id=1hVsTyC7DJH8pplo1DmVm13v8QDkONjwo\n",
      "From (redirected): https://drive.google.com/uc?export=download&id=1hVsTyC7DJH8pplo1DmVm13v8QDkONjwo&confirm=t&uuid=96127c73-18e7-40d0-bdac-89fa24bfdacb\n",
      "To: C:\\Users\\zehui\\review_list.json\n",
      "100%|██████████| 227M/227M [00:16<00:00, 13.8MB/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review_title</th>\n",
       "      <th>review_text</th>\n",
       "      <th>asin</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>helpful_vote</th>\n",
       "      <th>verified_purchase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>12 mg is 12 on the periodic table people! Mg f...</td>\n",
       "      <td>This review is more to clarify someone else’s ...</td>\n",
       "      <td>B07TDSJZMR</td>\n",
       "      <td>B07TDSJZMR</td>\n",
       "      <td>AFKZENTNBQ7A7V7UXW5JJI6UGRYQ</td>\n",
       "      <td>2020-02-06 00:49:35.902</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Save the lanet using less plastic.</td>\n",
       "      <td>Love these easy multitasking bleach tablets. B...</td>\n",
       "      <td>B08637FWWF</td>\n",
       "      <td>B08637FWWF</td>\n",
       "      <td>AEVWAM3YWN5URJVJIZZ6XPD2MKIA</td>\n",
       "      <td>2020-11-02 22:03:06.880</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Fantastic</td>\n",
       "      <td>I have been suffering a couple months with hee...</td>\n",
       "      <td>B07KJVGNN5</td>\n",
       "      <td>B07KJVGNN5</td>\n",
       "      <td>AHSPLDNW5OOUK2PLH7GXLACFBZNQ</td>\n",
       "      <td>2019-07-24 11:13:58.905</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>It holds the water and makes bubbles.  That's ...</td>\n",
       "      <td>It's cheap and it does what I wanted.  The \"ma...</td>\n",
       "      <td>B007HY7GC2</td>\n",
       "      <td>B092RP73CX</td>\n",
       "      <td>AEZGPLOYTSAPR3DHZKKXEFPAXUAA</td>\n",
       "      <td>2022-09-04 02:29:02.725</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Not for me</td>\n",
       "      <td>Didn't do a thing for me. Not saying they don'...</td>\n",
       "      <td>B08KYJLF5T</td>\n",
       "      <td>B08KYJLF5T</td>\n",
       "      <td>AEQAYV7RXZEBXMQIQPL6KCT2CFWQ</td>\n",
       "      <td>2022-01-20 23:53:07.262</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494116</th>\n",
       "      <td>5</td>\n",
       "      <td>Best brush!</td>\n",
       "      <td>Material is good. Worthy to buy.Firstly, the c...</td>\n",
       "      <td>B07KXT7Y48</td>\n",
       "      <td>B07KXT7Y48</td>\n",
       "      <td>AEQG5UEVYBNLWPXB3E2EODQ3EGSQ</td>\n",
       "      <td>2019-04-12 04:21:04.257</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494117</th>\n",
       "      <td>5</td>\n",
       "      <td>It makes my skin softer.</td>\n",
       "      <td>This brush is a good tool for cleaning and mas...</td>\n",
       "      <td>B07KXT7Y48</td>\n",
       "      <td>B07KXT7Y48</td>\n",
       "      <td>AGTVLNJAFZTKURBCHLUIH6VEOQCQ</td>\n",
       "      <td>2020-07-17 05:01:39.190</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494118</th>\n",
       "      <td>5</td>\n",
       "      <td>This brush is perfect !</td>\n",
       "      <td>Honestly, the brush totally is 15inch. Maybe y...</td>\n",
       "      <td>B07KXT7Y48</td>\n",
       "      <td>B07KXT7Y48</td>\n",
       "      <td>AGMA5UN3JPLQLQZ2PFYHJYSC4PNA</td>\n",
       "      <td>2019-03-07 22:33:36.968</td>\n",
       "      <td>173</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494119</th>\n",
       "      <td>5</td>\n",
       "      <td>Easy to use</td>\n",
       "      <td>Easy to hold, soft and flexible.</td>\n",
       "      <td>B07KXT7Y48</td>\n",
       "      <td>B07KXT7Y48</td>\n",
       "      <td>AE2Q3FXHIVGSSDGTNI4YLXDXMCIA</td>\n",
       "      <td>2019-09-26 12:22:51.244</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494120</th>\n",
       "      <td>5</td>\n",
       "      <td>La comunicación con la empresa y ez perfecto</td>\n",
       "      <td>Es como están en la foto</td>\n",
       "      <td>B07JYKP19X</td>\n",
       "      <td>B07JYKP19X</td>\n",
       "      <td>AH5K35CHSUC2QMGRMDQNXR6BJ72A</td>\n",
       "      <td>2019-10-03 01:03:59.311</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>494121 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        rating                                       review_title  \\\n",
       "0            4  12 mg is 12 on the periodic table people! Mg f...   \n",
       "1            5                 Save the lanet using less plastic.   \n",
       "2            5                                          Fantastic   \n",
       "3            4  It holds the water and makes bubbles.  That's ...   \n",
       "4            1                                         Not for me   \n",
       "...        ...                                                ...   \n",
       "494116       5                                        Best brush!   \n",
       "494117       5                           It makes my skin softer.   \n",
       "494118       5                            This brush is perfect !   \n",
       "494119       5                                        Easy to use   \n",
       "494120       5       La comunicación con la empresa y ez perfecto   \n",
       "\n",
       "                                              review_text        asin  \\\n",
       "0       This review is more to clarify someone else’s ...  B07TDSJZMR   \n",
       "1       Love these easy multitasking bleach tablets. B...  B08637FWWF   \n",
       "2       I have been suffering a couple months with hee...  B07KJVGNN5   \n",
       "3       It's cheap and it does what I wanted.  The \"ma...  B007HY7GC2   \n",
       "4       Didn't do a thing for me. Not saying they don'...  B08KYJLF5T   \n",
       "...                                                   ...         ...   \n",
       "494116  Material is good. Worthy to buy.Firstly, the c...  B07KXT7Y48   \n",
       "494117  This brush is a good tool for cleaning and mas...  B07KXT7Y48   \n",
       "494118  Honestly, the brush totally is 15inch. Maybe y...  B07KXT7Y48   \n",
       "494119                   Easy to hold, soft and flexible.  B07KXT7Y48   \n",
       "494120                           Es como están en la foto  B07JYKP19X   \n",
       "\n",
       "       parent_asin                       user_id               timestamp  \\\n",
       "0       B07TDSJZMR  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ 2020-02-06 00:49:35.902   \n",
       "1       B08637FWWF  AEVWAM3YWN5URJVJIZZ6XPD2MKIA 2020-11-02 22:03:06.880   \n",
       "2       B07KJVGNN5  AHSPLDNW5OOUK2PLH7GXLACFBZNQ 2019-07-24 11:13:58.905   \n",
       "3       B092RP73CX  AEZGPLOYTSAPR3DHZKKXEFPAXUAA 2022-09-04 02:29:02.725   \n",
       "4       B08KYJLF5T  AEQAYV7RXZEBXMQIQPL6KCT2CFWQ 2022-01-20 23:53:07.262   \n",
       "...            ...                           ...                     ...   \n",
       "494116  B07KXT7Y48  AEQG5UEVYBNLWPXB3E2EODQ3EGSQ 2019-04-12 04:21:04.257   \n",
       "494117  B07KXT7Y48  AGTVLNJAFZTKURBCHLUIH6VEOQCQ 2020-07-17 05:01:39.190   \n",
       "494118  B07KXT7Y48  AGMA5UN3JPLQLQZ2PFYHJYSC4PNA 2019-03-07 22:33:36.968   \n",
       "494119  B07KXT7Y48  AE2Q3FXHIVGSSDGTNI4YLXDXMCIA 2019-09-26 12:22:51.244   \n",
       "494120  B07JYKP19X  AH5K35CHSUC2QMGRMDQNXR6BJ72A 2019-10-03 01:03:59.311   \n",
       "\n",
       "        helpful_vote  verified_purchase  \n",
       "0                  3               True  \n",
       "1                  3               True  \n",
       "2                  0               True  \n",
       "3                  7               True  \n",
       "4                  0               True  \n",
       "...              ...                ...  \n",
       "494116             6               True  \n",
       "494117             0               True  \n",
       "494118           173               True  \n",
       "494119             0               True  \n",
       "494120             0               True  \n",
       "\n",
       "[494121 rows x 9 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "url = 'https://drive.google.com/uc?export=download&id=1hVsTyC7DJH8pplo1DmVm13v8QDkONjwo'\n",
    "output = 'review_list.json'\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "df_review = pd.read_json(output, lines=True)\n",
    "df_review = df_review.drop(columns=['images'])\n",
    "df_review.rename(columns={'title': 'review_title'}, inplace=True)\n",
    "df_review.rename(columns={'text': 'review_text'}, inplace=True)\n",
    "\n",
    "df_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "845f2cdf-fcae-4303-b259-0430817f13bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?export=download&id=14dVOPZM7-hCEdXJswxlb_c6XDErC7lac\n",
      "From (redirected): https://drive.google.com/uc?export=download&id=14dVOPZM7-hCEdXJswxlb_c6XDErC7lac&confirm=t&uuid=0d962b89-aac6-4ebb-8252-c9cca9b9e840\n",
      "To: C:\\Users\\zehui\\meta_list.json\n",
      "100%|██████████| 118M/118M [00:08<00:00, 13.4MB/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meta_title</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>rating_number</th>\n",
       "      <th>store</th>\n",
       "      <th>details</th>\n",
       "      <th>parent_asin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silicone Bath Body Brush Exfoliator Shower Bac...</td>\n",
       "      <td>3.9</td>\n",
       "      <td>7</td>\n",
       "      <td>Rzoeox</td>\n",
       "      <td>{'Package Dimensions': '15 x 3.3 x 1.5 inches;...</td>\n",
       "      <td>B07V346GZH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iPhone 7 Plus 8 Plus Screen Protector, ZHXIN T...</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2</td>\n",
       "      <td>ZHXIN</td>\n",
       "      <td>{'Brand': 'ZHXIN', 'Compatible Devices': 'Cell...</td>\n",
       "      <td>B075W927RH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Zig Zag Rolling Machine 70mm Size With FREE BO...</td>\n",
       "      <td>3.9</td>\n",
       "      <td>7</td>\n",
       "      <td>None</td>\n",
       "      <td>{'Package Dimensions': '4.1 x 1.8 x 0.3 inches...</td>\n",
       "      <td>B01FB26VKY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sting-Kill Disposable Wipes 8 Each ( Pack of 5)</td>\n",
       "      <td>4.1</td>\n",
       "      <td>6</td>\n",
       "      <td>Sting-kill</td>\n",
       "      <td>{'Brand': 'Sting-kill', 'Item Form': 'Wipe', '...</td>\n",
       "      <td>B01IAI29RU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Heated Eyelash Curler Mini Portable Electric E...</td>\n",
       "      <td>3.3</td>\n",
       "      <td>8</td>\n",
       "      <td>BiBOSS</td>\n",
       "      <td>{'Package Dimensions': '6.1 x 3.1 x 1.9 inches...</td>\n",
       "      <td>B08CMN38RC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60288</th>\n",
       "      <td>5th Scooby Doo Party Supplies | Decorations | ...</td>\n",
       "      <td>4.1</td>\n",
       "      <td>5</td>\n",
       "      <td>JUMPHOP</td>\n",
       "      <td>{'Package Dimensions': '14.5 x 10.5 x 1.5 inch...</td>\n",
       "      <td>B08VVZRBBL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60289</th>\n",
       "      <td>Crystal Hair Eraser for Hair Removal, Crystal ...</td>\n",
       "      <td>2.3</td>\n",
       "      <td>10</td>\n",
       "      <td>PARCO</td>\n",
       "      <td>{}</td>\n",
       "      <td>B0BZQH4WGD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60290</th>\n",
       "      <td>Designer Modern Ball Acrylic Chair with Black ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Designer Modern</td>\n",
       "      <td>{'Package Dimensions': '50 x 45 x 45 inches; 5...</td>\n",
       "      <td>B00A6CANPY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60291</th>\n",
       "      <td>Biotics Research - UltraVir-X 90C (2 Pack)</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>BIOTICS</td>\n",
       "      <td>{'Is Discontinued By Manufacturer': 'No', 'Dat...</td>\n",
       "      <td>B06W57751V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60292</th>\n",
       "      <td>Naturally Clean Fruit and Veggie Wash, 17 oz S...</td>\n",
       "      <td>4.4</td>\n",
       "      <td>3</td>\n",
       "      <td>Fladora</td>\n",
       "      <td>{'Product Dimensions': '4 x 4 x 4 inches; 1.06...</td>\n",
       "      <td>B07T2FYNYS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60293 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              meta_title  average_rating  \\\n",
       "0      Silicone Bath Body Brush Exfoliator Shower Bac...             3.9   \n",
       "1      iPhone 7 Plus 8 Plus Screen Protector, ZHXIN T...             3.8   \n",
       "2      Zig Zag Rolling Machine 70mm Size With FREE BO...             3.9   \n",
       "3        Sting-Kill Disposable Wipes 8 Each ( Pack of 5)             4.1   \n",
       "4      Heated Eyelash Curler Mini Portable Electric E...             3.3   \n",
       "...                                                  ...             ...   \n",
       "60288  5th Scooby Doo Party Supplies | Decorations | ...             4.1   \n",
       "60289  Crystal Hair Eraser for Hair Removal, Crystal ...             2.3   \n",
       "60290  Designer Modern Ball Acrylic Chair with Black ...             5.0   \n",
       "60291         Biotics Research - UltraVir-X 90C (2 Pack)             5.0   \n",
       "60292  Naturally Clean Fruit and Veggie Wash, 17 oz S...             4.4   \n",
       "\n",
       "       rating_number            store  \\\n",
       "0                  7           Rzoeox   \n",
       "1                  2            ZHXIN   \n",
       "2                  7             None   \n",
       "3                  6       Sting-kill   \n",
       "4                  8           BiBOSS   \n",
       "...              ...              ...   \n",
       "60288              5          JUMPHOP   \n",
       "60289             10            PARCO   \n",
       "60290              2  Designer Modern   \n",
       "60291              2          BIOTICS   \n",
       "60292              3          Fladora   \n",
       "\n",
       "                                                 details parent_asin  \n",
       "0      {'Package Dimensions': '15 x 3.3 x 1.5 inches;...  B07V346GZH  \n",
       "1      {'Brand': 'ZHXIN', 'Compatible Devices': 'Cell...  B075W927RH  \n",
       "2      {'Package Dimensions': '4.1 x 1.8 x 0.3 inches...  B01FB26VKY  \n",
       "3      {'Brand': 'Sting-kill', 'Item Form': 'Wipe', '...  B01IAI29RU  \n",
       "4      {'Package Dimensions': '6.1 x 3.1 x 1.9 inches...  B08CMN38RC  \n",
       "...                                                  ...         ...  \n",
       "60288  {'Package Dimensions': '14.5 x 10.5 x 1.5 inch...  B08VVZRBBL  \n",
       "60289                                                 {}  B0BZQH4WGD  \n",
       "60290  {'Package Dimensions': '50 x 45 x 45 inches; 5...  B00A6CANPY  \n",
       "60291  {'Is Discontinued By Manufacturer': 'No', 'Dat...  B06W57751V  \n",
       "60292  {'Product Dimensions': '4 x 4 x 4 inches; 1.06...  B07T2FYNYS  \n",
       "\n",
       "[60293 rows x 6 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://drive.google.com/uc?export=download&id=14dVOPZM7-hCEdXJswxlb_c6XDErC7lac'\n",
    "output = 'meta_list.json'\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "df_meta = pd.read_json(output, lines=True)\n",
    "df_meta = df_meta.drop(columns=['main_category','features','description','price','images','videos','categories','bought_together'])\n",
    "df_meta.rename(columns={'title': 'meta_title'}, inplace=True)\n",
    "\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4435da46-1345-4f84-b790-a412df064a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review['datetime'] = pd.to_datetime(df_review['timestamp'], unit='ms')\n",
    "\n",
    "df_review['review_text_word_count'] = df_review['review_text'].apply(lambda x: len(x.split()) if isinstance(x, str) else 0)\n",
    "df_review['review_text_word_count_cutoff'] = df_review['review_text_word_count'].apply(lambda x: x if x <= 300 else 300)\n",
    "df_review['review_title_word_count'] = df_review['review_title'].apply(lambda x: len(x.split()) if isinstance(x, str) else 0)\n",
    "\n",
    "df_meta['meta_title_word_count'] = df_meta['meta_title'].apply(lambda x: len(x.split()) if isinstance(x, str) else 0)\n",
    "df_meta['average_rating_threshold'] = np.ceil(df_meta['average_rating'] * 2) / 2\n",
    "\n",
    "df_review['timestamp'] = pd.to_datetime(df_review['timestamp'])\n",
    "df_review['year'] = df_review['timestamp'].dt.year\n",
    "df_review['month'] = df_review['timestamp'].dt.month\n",
    "df_review['day'] = df_review['timestamp'].dt.day\n",
    "df_review['weekday'] = df_review['timestamp'].dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb98eb52-e766-4a17-aeef-d207f227c4a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_df = pd.merge(df_review, df_meta, on='parent_asin', how='left')\n",
    "merged_df = merged_df.sample(frac=0.02, random_state=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a32cd2f6-3a8a-4901-bc9f-b2c709c31ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zehui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\zehui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zehui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\zehui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\zehui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\zehui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\zehui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Tocknize - Text Handling - Prepare\n",
    "import re\n",
    "import nltk\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from contractions import fix\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('omw-1.4')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3bd69ce-4142-43e7-90f5-ff647f3dec9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatizer Function\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#Stemming Function\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "#Clean and tockenize function\n",
    "def clean_and_tokenize(text):\n",
    "\n",
    "    # Expanding contractions (e.g., \"don't\" → \"do not\")\n",
    "    text = fix(text)\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Removing multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Removing punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Removing emojis\n",
    "    text = emoji.replace_emoji(text, replace='')\n",
    "    # Removing URLs\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "    # Removing email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    # Removing stop words\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    # Removing extra short words (e.g., \"a\", \"an\", \"it\")\n",
    "    text = ' '.join([word for word in text.split() if len(word) > 1])\n",
    "\n",
    "    words = tokenizer.tokenize(text)  # Tokenization\n",
    "    return words\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.VERB)\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "#merged_df[\"tokenized_review_title\"] = merged_df[\"review_title\"].apply(clean_and_tokenize)\n",
    "#merged_df[\"tokenized_review_text\"] = merged_df[\"review_text\"].apply(clean_and_tokenize)\n",
    "\n",
    "#merged_df[\"tokenized_review_title_lem\"] = merged_df[\"tokenized_review_title\"].apply(lemmatize_tokens)\n",
    "#merged_df[\"tokenized_review_text_lem\"] = merged_df[\"tokenized_review_text\"].apply(lemmatize_tokens)\n",
    "\n",
    "#merged_df[\"tokenized_review_title_stem\"] = merged_df[\"tokenized_review_title\"].apply(stem_tokens)\n",
    "#merged_df[\"tokenized_review_text_stem\"] = merged_df[\"tokenized_review_text\"].apply(stem_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1f58b6-7170-4e43-b814-5ed65cbedca3",
   "metadata": {},
   "source": [
    "## Everything above was our regular step for data cleaning. Topic modeling starts here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a293d3f8-2d01-4c60-99aa-d0968406aeaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\zehui\\anaconda3\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.49.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.29.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
      "Requirement already satisfied: spacy in c:\\users\\zehui\\anaconda3\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from spacy) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ------- -------------------------------- 2.4/12.8 MB 12.2 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 14.0 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 7.9/12.8 MB 12.8 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 10.7/12.8 MB 13.2 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 12.7 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e56e372-84ea-480e-a612-62ff488a19f8",
   "metadata": {},
   "source": [
    "#### The cell below is used to prepare merged_df['meta_title'] for topic modeling. The list of words we chose to drop was updated multiple times based on our results later. The main criterion for dropping these words is to retain only those that truly indicate the type of product. For example, if a product name is \"fish oil for man\", we'd only keep \"fish oil\" to reduce noise — for subcategory classification, we're only interested in the product itself. Therefore, we removed common stop words, descriptive words like \"premium\" and color terms, as well as special characters. We also applied lemmatization to return words to their original forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "097e264f-da25-4f32-895f-31199439ccee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "stopwords = {\"and\", \"&\", \"|\", \"-\", \"for\", \"with\", \"of\"}\n",
    "common_units = {\"mg\", \"g\", \"kg\", \"lb\", \"oz\", \"ml\", \"l\", \"fl\", \"inch\", \"cm\", \"mm\",\n",
    "                \"count\", \"pcs\", \"pack\", \"set\", \"ct\", \"bottle\", \"bag\"}\n",
    "custom_stopwords = {\n",
    "    \"the\", \"a\", \"is\", \"in\", \"on\", \"for\", \"to\", \"by\", \"this\", \"with\", \"your\", \n",
    "    \"it\", \"as\", \"no\", \"or\", \"all\", \"at\", \"be\", \"an\", \"we\", \"that\", \"can\", \"so\", \"you\",\n",
    "    \"one\", \"from\", \"us\", \"more\", \"other\", \"will\", \"not\", \"any\", \"than\", \"some\", \"get\",\n",
    "    \"each\", \"our\", \"if\", \"over\", \"when\", \"their\", \"do\", \"out\", \"after\", \"before\",\n",
    "    \"back\", \"my\", \"me\", \"her\", \"his\", \"them\", \"they\", \"him\", \"she\", \"he\"}\n",
    "\n",
    "more_words = {\"premium\", \"professional\", \"plus\", \"use\", \"large\", \"high\", \"size\", \n",
    "              \"make\", \"easy\", \"white\", \"black\", \"grey\", \"gray\", \"silver\", \"gold\", \"golden\", \"blue\", \"green\"\n",
    "              \"red\", \"yellow\", \"purple\", \"pink\", \"party\", \"kit\", \"make\", \"long\", \"use\", \"birthday\", \"woman\", \"man\", \"kid\"}\n",
    "\n",
    "all_stopwords = stopwords | common_units | custom_stopwords | more_words\n",
    "\n",
    "def clean_meta_title(title, do_lemmatize=True):\n",
    "\n",
    "    title = title.lower().strip()\n",
    "    title = re.sub(r'[^\\w\\s]', '', title)\n",
    "    title = re.sub(r'[^a-zA-Z\\s]', '', title)\n",
    "    title = re.sub(r'\\s+', ' ', title).strip()\n",
    "    words = title.split()\n",
    "    cleaned_words = [word for word in words if word not in all_stopwords]\n",
    "\n",
    "    if do_lemmatize:\n",
    "        doc = nlp(\" \".join(cleaned_words))\n",
    "        lemmatized = []\n",
    "        for token in doc:\n",
    "            if len(token.lemma_) > 2:\n",
    "                lemmatized.append(token.lemma_)\n",
    "        cleaned_words = lemmatized\n",
    "    \n",
    "    cleaned_words = [w for w in cleaned_words if len(w) > 1]\n",
    "\n",
    "    return \" \".join(cleaned_words)\n",
    "\n",
    "merged_df[\"cleaned_meta_title\"] = merged_df[\"meta_title\"].apply(lambda x: clean_meta_title(x, do_lemmatize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1a7411-d0f5-4482-9818-72d170e8e3a8",
   "metadata": {},
   "source": [
    "#### From our later analysis, we noticed that some product title keywords like \"balloon\", \"wedding\", and pet-related terms appeared unusually frequently. These words are not necessarily related to our \"Health and Personal Care\" category. We believe these products may have been miscategorized by Amazon sellers. Therefore, we decided to drop these products from our dataset and focus only on those products that are truly relevant to our category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e609b407-d9d6-422b-94c5-cc827dcc0438",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = {\"gift\", \"decoration\", \"wedding\", \"balloon\", \"cat\", \"dog\", \"pet\"}\n",
    "\n",
    "def contains_keywords(text):\n",
    "    tokens = text.split()\n",
    "    return any(word in tokens for word in keywords)\n",
    "\n",
    "merged_df = merged_df[~merged_df[\"cleaned_meta_title\"].apply(contains_keywords)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b528732-be04-4bbe-8203-faaab8432669",
   "metadata": {},
   "source": [
    "#### We applied NMF using both BoW and TF-IDF vectorization to compare the top 20 words in each subtopic by frequency. The printouts from the two approaches revealed that while both methods generally helped NMF classify products into subcategories, TF-IDF produced more coherent and well-defined topics overall. For instance, topics related to orthopedic supports, nutritional supplements, hair styling, and foot care were more focused with TF-IDF. In contrast, the BoW approach tended to include less informative words (e.g., \"dead\", \"come\") and mixed signals—such as blending fish oil products with words like \"scale\". Furthermore, TF-IDF gives greater influence to unique, category-specific terms while reducing noise from common words. \n",
    "#### Therefore, we decided to use TF-IDF vectorization for NMF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "09fc158d-04f1-435f-8901-b81a10001863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: clean, natural, mat, organic, cleaning, safe, gym, towel, gear, restore, yoga, equipment, residue, fitness, aroma, come, lemongrass, wmicrofiber, refreshe, slippery\n",
      "Topic 2: foot, callus, remover, pedicure, file, shoe, heel, skin, tool, dry, massager, dead, crack, spa, scrubber, massage, pumice, odor, insole, stone\n",
      "Topic 3: hair, iron, straightener, ceramic, straighten, curler, brush, trimmer, flat, styling, curl, comb, straight, fast, detangle, growth, cordless, shaver, anion, natural\n",
      "Topic 4: oil, fish, liquid, essential, supplement, lemon, pure, organic, fine, source, omegas, norwegian, carlson, sustainably, wildcaught, very, scale, dropper, peppermint, usda\n",
      "Topic 5: neck, pain, relief, support, brace, adjustable, traction, shoulder, posture, cervical, device, corrector, provide, stretcher, therapy, upper, muscle, knee, heat, inflatable\n",
      "Topic 6: brush, electric, facial, head, body, toothbrush, skin, scrubber, sponge, shower, rechargeable, bath, care, massager, shaver, waterproof, portable, replacement, face, massage\n",
      "Topic 7: mask, face, ear, sleep, adjustable, eye, strap, reusable, adult, holder, comfortable, lanyard, cover, noise, chain, plug, washable, protection, cloth, travel\n",
      "Topic 8: pill, powder, supplement, vitamin, organizer, tooth, box, organic, capsule, case, day, support, travel, free, natural, container, protein, flavor, vegan, medicine\n",
      "Topic 9: pad, glass, shoe, heating, pair, heel, eyeglass, cushion, nose, soft, eyelash, wipe, scale, silicone, curler, pain, heat, therapy, adhesive, lens\n",
      "Topic 10: nail, file, glass, manicure, tool, acrylic, clipper, buffer, pedicure, board, natural, grit, emery, art, double, steel, stainless, cutter, crystal, toenail\n"
     ]
    }
   ],
   "source": [
    "#BoW NMF\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    max_df=0.5, \n",
    "    min_df=10, \n",
    "    stop_words=list(all_stopwords)\n",
    ")\n",
    "bow = vectorizer.fit_transform(merged_df[\"cleaned_meta_title\"])\n",
    "\n",
    "num_topics = 10 \n",
    "nmf_model_bow = NMF(\n",
    "    n_components=num_topics, \n",
    "    random_state=99,           \n",
    "    l1_ratio=0.5, \n",
    "    max_iter=500\n",
    ")\n",
    "\n",
    "nmf_topics_bow = nmf_model_bow.fit_transform(bow)\n",
    "\n",
    "feature_names_bow = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(nmf_model_bow.components_):\n",
    "    top_words = [feature_names_bow[i] for i in topic.argsort()[:-21:-1]]\n",
    "    print(f\"Topic {topic_idx+1}: {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eb873e-4e35-4ddb-baf0-80508ab16c45",
   "metadata": {},
   "source": [
    "### THIS IS OUR FINAL METHOD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f933699f-73a0-4694-9fc3-f5cc9a6bb8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: sponge, silicone, makeup, brush, body, shower, facial, bath, scrubber, skin, loofah, exfoliate, massager, scrub, cosmetic, latex, care, handle, soft, lotion\n",
      "Topic 2: clean, mat, natural, cleaning, towel, restore, mindful, refreshe, deepcleanse, asutra, wmicrofiber, slippery, lemongrass, residue, yoga, fitness, come, aroma, gym, gear\n",
      "Topic 3: pain, relief, support, neck, brace, posture, shoulder, corrector, adjustable, pad, upper, provide, therapy, knee, clavicle, compression, heating, cervical, muscle, traction\n",
      "Topic 4: oil, organic, pure, essential, powder, cap, usda, dropper, supplement, certify, peppermint, available, wimprove, variation, fish, natural, capsule, liquid, vitamin, pill\n",
      "Topic 5: hair, iron, straightener, ceramic, curler, brush, straighten, trimmer, flat, curl, styling, comb, shaver, straight, fast, electric, tool, eyelash, heating, salon\n",
      "Topic 6: design, cushion, pressure, comfort, chair, grid, gaming, ultimate, usa, reduce, travel, relieve, seat, charcoal, backnobber, positive, tooth, foam, ergonomic, water\n",
      "Topic 7: foot, file, remover, callus, heel, pedicure, shoe, electronic, nail, tool, insole, skin, spa, massage, pair, massager, pad, dead, crack, roller\n",
      "Topic 8: air, odor, guardian, uvc, sanitizer, deodorizerkill, petssmokemoldcooke, germsfreshen, germguardian, laundrypack, ggpk, pluggable, technology, reduce, scale, spray, freshener, bathroom, digital, eliminator\n",
      "Topic 9: mask, face, glass, ear, adjustable, box, sleep, adult, reusable, pill, case, cotton, cover, cloth, comfortable, washable, holder, scale, strap, buttonsmith\n",
      "Topic 10: toothbrush, head, replacement, electric, brush, compatible, rechargeable, tooth, water, oral, shaver, sonic, mode, waterproof, battery, dental, cordless, flosser, sonicare, usb\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF NMF \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.5, \n",
    "    min_df=10, \n",
    "    stop_words=list(all_stopwords),\n",
    ")\n",
    "tfidf = vectorizer.fit_transform(merged_df[\"cleaned_meta_title\"])\n",
    "\n",
    "\n",
    "num_topics = 10\n",
    "nmf_model = NMF(\n",
    "    n_components=num_topics, \n",
    "    random_state=99,\n",
    "    l1_ratio=0.5, \n",
    "    max_iter=500\n",
    ")\n",
    "nmf_topics = nmf_model.fit_transform(tfidf)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(nmf_model.components_):\n",
    "    top_words = [feature_names[i] for i in topic.argsort()[:-21:-1]]\n",
    "    print(f\"Topic {topic_idx+1}: {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e85b8a0d-d4a7-4e27-9dfb-802be6d43579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dominant_topic\n",
      "0    0.068079\n",
      "1    0.068940\n",
      "2    0.089159\n",
      "3    0.213917\n",
      "4    0.056141\n",
      "5    0.027963\n",
      "6    0.105399\n",
      "7    0.024521\n",
      "8    0.223381\n",
      "9    0.122499\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Distribution of TF-IDF NMF Topics \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "dominant_topics = np.argmax(nmf_topics, axis=1)\n",
    "\n",
    "merged_df[\"dominant_topic\"] = dominant_topics\n",
    "\n",
    "topic_distribution = merged_df[\"dominant_topic\"].value_counts(normalize=True).sort_index()\n",
    "print(topic_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc57e9f-875e-4a35-bf6b-fdee360c21f9",
   "metadata": {},
   "source": [
    "#### We also tried the LDA method. Below is our preparation for LDA. In addition to the general cleaning described above, we also attempted to merge frequently appearing words (based on our understanding of the domain) to increase the interpretability of our output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2afeb72b-f330-4be1-bf98-43688c2b064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"tokens\"] = merged_df[\"cleaned_meta_title\"].apply(lambda x: x.split())\n",
    "\n",
    "phrases_to_merge = [\n",
    "    (\"tooth\", \"brush\"),\n",
    "    (\"dental\", \"care\"),\n",
    "    (\"oral\", \"care\"),\n",
    "    (\"electric\", \"toothbrush\"),\n",
    "    (\"tooth\", \"paste\"),\n",
    "    (\"fish\", \"oil\"),\n",
    "    (\"face\", \"mask\"),\n",
    "    (\"face\", \"cream\"),\n",
    "    (\"facial\", \"cleanser\"),\n",
    "    (\"face\", \"wash\"),\n",
    "    (\"facial\", \"mask\"),\n",
    "    (\"skin\", \"care\"),\n",
    "    (\"skin\", \"moisturizer\"),\n",
    "    (\"facial\", \"serum\"),\n",
    "    (\"skin\", \"toner\"),\n",
    "    (\"hair\", \"brush\"),\n",
    "    (\"hair\", \"dryer\"),\n",
    "    (\"hair\", \"straightener\"),\n",
    "    (\"hair\", \"treatment\"),\n",
    "    (\"hair\", \"removal\"),\n",
    "    (\"body\", \"lotion\"),\n",
    "    (\"bath\", \"bomb\"),\n",
    "    (\"shower\", \"gel\"),\n",
    "    (\"body\", \"wash\"),\n",
    "    (\"body\", \"scrub\"),\n",
    "    (\"pain\", \"relief\"),\n",
    "    (\"weight\", \"loss\"),\n",
    "    (\"blood\", \"pressure\"),\n",
    "    (\"vitamin\", \"supplement\"),\n",
    "    (\"supplement\", \"capsule\"),\n",
    "    (\"nail\", \"polish\"),\n",
    "    (\"nail\", \"file\"),\n",
    "    (\"foot\", \"care\"),\n",
    "    (\"massage\", \"chair\"),\n",
    "    (\"sunscreen\", \"lotion\"),\n",
    "    (\"anti\", \"aging\")\n",
    "]\n",
    "phrase_set = set(phrases_to_merge)\n",
    "\n",
    "def merge_phrases(doc, phrase_tuples):\n",
    "    i = 0\n",
    "    merged_doc = []\n",
    "    while i < len(doc):\n",
    "        if i < len(doc) - 1 and (doc[i], doc[i+1]) in phrase_tuples:\n",
    "            merged_doc.append(f\"{doc[i]}_{doc[i+1]}\")\n",
    "            i += 2 \n",
    "        else:\n",
    "            merged_doc.append(doc[i])\n",
    "            i += 1\n",
    "    return merged_doc\n",
    "\n",
    "merged_df[\"tokens\"] = merged_df[\"tokens\"].apply(lambda doc: merge_phrases(doc, phrase_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9236950f-f5c9-49b6-b207-6ce8993b6759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary = Dictionary(merged_df[\"tokens\"])\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.8)\n",
    "bow_corpus = [dictionary.doc2bow(tokens) for tokens in merged_df[\"tokens\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61d3c36-9e05-44fd-82f3-b11c86468741",
   "metadata": {},
   "source": [
    "#### When performing LDA, we used the LDA Coherence score to assess the performance of our model. A higher score means more semantically consistent, which means cleaner separation. We learned from ChatGPT that a score above 0.4 is generally acceptable. At the end of this notebook, please find a full record of our LDA hyperparameter tuning, which took hours to complete. We looped over alpha = [0.01, 0.1, 1], passes = [10, 20, 30], and tested 10–30 topics for each hyperparameter combination. From the output, we found that the best set of hyperparameters is [alpha = 1.0, passes = 10, num_topics = 28, Coherence = 0.4716]. However, we think 28 topics is too many for our analysis, so we eventually decided to go with num_topics = 11, which generally received a high coherence score across different passes and alpha values.\n",
    "\n",
    "#### When we were hyper-tuning and interpreting the printout of LDA-separated topics’ high-frequency words, we found there is a general trade-off between coherence score and interpretability. We once reached a score as high as 0.68, but the printed words included uncleaned stopwords and some single alphabets. The final parameters we chose resulted in an LDA score of 0.4152, which just passed the acceptable standard for LDA coherence. We found that the topics it segregated are overall interpretable, though some topics are mixed and overlapping. For example, Topic 6 (toothbrush, head, non, replacement, air, muscle, cap, reduce, technology, odor, gmo, base, pro, uvc, sanitizer, guardian, pluggable, ggpk, petssmokemoldcooke, laundrypack) appears to be a mixture of oral, odor, and germ-related topics.\n",
    "\n",
    "#### We also compared the coherence score of NMF and LDA. With a score of 0.5447, more interpretable topic segregation, and a much shorter computing time, NMF is clearly the better choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0ba5d166-ef3c-445d-8e28-73846521b06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "num_topics = 11 \n",
    "lda_model = LdaModel(\n",
    "    corpus=bow_corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=num_topics,\n",
    "    random_state=42,\n",
    "    passes=10,\n",
    "    iterations=400,\n",
    "    alpha= 0.01,\n",
    "    per_word_topics=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7f07a511-a0d9-4b01-a731-c3c857661c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: brush, facial, body, shower, sponge, bath, glass, clean, scrubber, head, replacement, electric, steel, stainless, face, handle, wipe, woman, beard, clipper\n",
      "Topic 1: foot, massage, massager, roller, remover, file, callus, spa, skin, pedicure, solution, care, clean, muscle, deep, dry, tool, heat, value, clear\n",
      "Topic 2: color, light, ultra, red, free, water, medical, nail, filter, bottle, supply, original, paper, probiotic, repair, fit, tablet, patch, baby, cup\n",
      "Topic 3: hair, ear, iron, noise, sleep, ceramic, flat, machine, plug, air, hair_straightener, portable, sound, snore, straighten, plantar, fasciitis, brush, fast, scale\n",
      "Topic 4: supplement, powder, capsule, support, organic, vitamin, natural, health, energy, liquid, fish_oil, vegan, formula, hair, strength, flavor, joint, nongmo, extract, free\n",
      "Topic 5: clean, protein, mat, natural, cleaning, bar, organic, towel, cloth, fitness, bag, safe, restore, equipment, reusable, yoga, aroma, gear, residue, pad\n",
      "Topic 6: toothbrush, non, air, head, muscle, cap, reduce, technology, odor, base, gmo, uvc, sanitizer, pro, guardian, ggpk, germsfreshen, germguardian, petssmokemoldcooke, laundrypack\n",
      "Topic 7: mask, compression, glass, sock, holder, strap, bandage, woman, adjustable, support, adult, man, ankle, eyeglass, sleeve, cotton, face_mask, tissue, nail_file, knee\n",
      "Topic 8: oil, tooth, essential, organic, natural, water, spray, whiten, pure, scale, cold, shoe, digital, repellent, electrolyte, peppermint, bathroom, moisturizing, dropper, powder\n",
      "Topic 9: pill, box, design, case, battery, travel, glove, organizer, ounce, portable, make, pressure, day, reduce, plastic, usa, comfort, pump, cushion, free\n",
      "Topic 10: pad, woman, man, neck, pain_relief, support, pain, brace, adjustable, eyelash, shoe, shoulder, foot, heel, gel, electric, insole, pair, posture, eye\n"
     ]
    }
   ],
   "source": [
    "for i, topic in lda_model.show_topics(num_topics=num_topics, num_words=20, formatted=False):\n",
    "    top_words = [word for word, _ in topic]\n",
    "    print(f\"Topic {i}: {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "af11e179-8866-4426-a4fe-c19a0c50859f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Coherence: 0.4152\n",
      "NMF Coherence: 0.5447\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# LDA Score\n",
    "coherence_model_lda = CoherenceModel(\n",
    "    model=lda_model, \n",
    "    texts=merged_df[\"tokens\"], \n",
    "    dictionary=dictionary, \n",
    "    coherence='c_v'\n",
    ")\n",
    "coherence_score = coherence_model_lda.get_coherence()\n",
    "print(f\"LDA Coherence: {coherence_score:.4f}\")\n",
    "\n",
    "# NMF Score\n",
    "def get_nmf_topics(nmf_model, feature_names, topn=20):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(nmf_model.components_):\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[:-topn-1:-1]]\n",
    "        topics.append(top_words)\n",
    "    return topics\n",
    "\n",
    "nmf_topics_words = get_nmf_topics(nmf_model, feature_names, topn=20)\n",
    "\n",
    "nmf_coherence_model = CoherenceModel(topics=nmf_topics_words, texts=merged_df[\"tokens\"].tolist(), dictionary=dictionary, coherence='c_v')\n",
    "nmf_coherence = nmf_coherence_model.get_coherence()\n",
    "print(f\"NMF Coherence: {nmf_coherence:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17be2229-e94b-4268-9ec5-23dabdc551ad",
   "metadata": {},
   "source": [
    "#### Besides NMF and LDA, we also performed BERTopic, according to ChatGPT's suggestion. However, BERTopic does not allow us to set the number of topics manually, and it produced 297 topics for our data. We tried to reduce the topics to 10, but the reduced topics resulted in an extremely unbalanced distribution—Topic 1 contains about 1,500 products and Topic 2 contains about 7,000 products, while the remaining topics average around 100 products. This shows that BERTopic did not perform well when the number of topics was reduced to a small number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "94a93b54-4632-44a1-8ac7-5102da3983ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bertopic in c:\\users\\zehui\\anaconda3\\lib\\site-packages (0.16.4)\n",
      "Requirement already satisfied: hdbscan>=0.8.29 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from bertopic) (0.8.40)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from bertopic) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.1.5 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from bertopic) (2.2.2)\n",
      "Requirement already satisfied: plotly>=4.7.0 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from bertopic) (5.24.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2.post1 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from bertopic) (1.5.1)\n",
      "Requirement already satisfied: sentence-transformers>=0.4.1 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from bertopic) (3.4.1)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from bertopic) (4.66.5)\n",
      "Requirement already satisfied: umap-learn>=0.5.0 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from bertopic) (0.5.7)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.29->bertopic) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.29->bertopic) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2023.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from plotly>=4.7.0->bertopic) (8.2.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from plotly>=4.7.0->bertopic) (24.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.5.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (4.49.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (2.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (0.29.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (10.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from tqdm>=4.41.1->bertopic) (0.4.6)\n",
      "Requirement already satisfied: numba>=0.51.2 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from umap-learn>=0.5.0->bertopic) (0.60.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from umap-learn>=0.5.0->bertopic) (0.5.13)\n",
      "Requirement already satisfied: filelock in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2024.6.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (4.11.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.43.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.16.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.5.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zehui\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "99a67817-6609-4370-99e6-135b7197680a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-01 22:32:12,674 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b80042b9384db8b05900f0d0fe78ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-01 22:35:12,909 - BERTopic - Embedding - Completed ✓\n",
      "2025-03-01 22:35:12,911 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-03-01 22:36:07,276 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-03-01 22:36:07,279 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-03-01 22:36:08,248 - BERTopic - Cluster - Completed ✓\n",
      "2025-03-01 22:36:08,253 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2025-03-01 22:36:09,064 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Topic  Count                                        Name  \\\n",
      "0       -1   1692                        -1_cleaner_by_kit_of   \n",
      "1        0    182            0_iron_straightener_ceramic_hair   \n",
      "2        1    143           1_capsules_moringa_120_vegetarian   \n",
      "3        2    136         2_reading_readers_glasses_eyekepper   \n",
      "4        3    129               3_pill_organizer_box_medicine   \n",
      "..     ...    ...                                         ...   \n",
      "283    282     11        282_whitening_teeth_combine_enhances   \n",
      "284    283     11           283_feets_unbleached_japan_sliver   \n",
      "285    284     10  284_hyaluronic_acid_lubrication_naturebell   \n",
      "286    285     10                  285_brushing_dry_boar_skin   \n",
      "287    286     10                      286_mask_kpop_hole_exo   \n",
      "\n",
      "                                        Representation  \\\n",
      "0    [cleaner, by, kit, of, energy, the, oz, capsul...   \n",
      "1    [iron, straightener, ceramic, hair, curling, c...   \n",
      "2    [capsules, moringa, 120, vegetarian, extract, ...   \n",
      "3    [reading, readers, glasses, eyekepper, bifocal...   \n",
      "4    [pill, organizer, box, medicine, weekly, day, ...   \n",
      "..                                                 ...   \n",
      "283  [whitening, teeth, combine, enhances, pens, wa...   \n",
      "284  [feets, unbleached, japan, sliver, japanese, 1...   \n",
      "285  [hyaluronic, acid, lubrication, naturebell, ia...   \n",
      "286  [brushing, dry, boar, skin, brush, bristle, ex...   \n",
      "287  [mask, kpop, hole, exo, face, earloop, masks, ...   \n",
      "\n",
      "                                   Representative_Docs  \n",
      "0    [Foot Scrubber Callus Remover for Feet : Pedic...  \n",
      "1    [Hair Straightener, 2 in 1 Straightener and Cu...  \n",
      "2    [Organic Forskolin Extract Capsules 5000MG wit...  \n",
      "3    [BBI The Foster Bifocal Sun Reader Sport and W...  \n",
      "4    [Pill Organizer 4 Times A Day, BUG HULL Moistu...  \n",
      "..                                                 ...  \n",
      "283  [Teeth Whitening LED Light Enhances Tooth Whit...  \n",
      "284  [Japanese Organic Cotton Sliver 13 Feets 100% ...  \n",
      "285  [NatureBell 2 Pack Hyaluronic Acid Supplements...  \n",
      "286  [Dry Brushing Body Brush Skin Brush Dry Brush ...  \n",
      "287  [ZUKIBO Mask Drink Hole Straw Hole for Mask Re...  \n",
      "\n",
      "[288 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "documents = merged_df[\"meta_title\"].tolist()\n",
    "topic_model = BERTopic(verbose=True)\n",
    "topics, probabilities = topic_model.fit_transform(documents)\n",
    "\n",
    "topic_info = topic_model.get_topic_info()\n",
    "print(topic_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "11627cc6-b11b-4142-a2cf-c3e4e98af230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "topic_model = BERTopic()\n",
    "topics, probs = topic_model.fit_transform(documents)\n",
    "\n",
    "topic_model = topic_model.reduce_topics(docs=documents, nr_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ccef2d46-6462-4277-a2a1-eb5112e3f4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topic  Count                                     Name  \\\n",
      "0     -1   1617                     -1_for_and_pack_with   \n",
      "1      0   6928                      0_for_and_with_pack   \n",
      "2      1    270          1_scale_digital_weight_bathroom   \n",
      "3      2    192                  2_ear_plugs_noise_sound   \n",
      "4      3     93                  3_battery_lithium_3d_3v   \n",
      "5      4     88              4_backnobber_positive_co_ii   \n",
      "6      5     47       5_smokebuddy_buddy_smoke_cigarette   \n",
      "7      6     38            6_nerdwax_slipping_shark_seen   \n",
      "8      7     13                 7_sq_stretchtite_3000_ft   \n",
      "9      8     12  8_beard_application_horsehair_handlebar   \n",
      "\n",
      "                                      Representation  \\\n",
      "0  [for, and, pack, with, of, support, oz, foot, ...   \n",
      "1  [for, and, with, pack, oz, of, hair, natural, ...   \n",
      "2  [scale, digital, weight, bathroom, smart, glas...   \n",
      "3  [ear, plugs, noise, sound, concerts, reduction...   \n",
      "4  [battery, lithium, 3d, 3v, micropedi, emjoi, t...   \n",
      "5  [backnobber, positive, co, ii, charcoal, the, ...   \n",
      "6  [smokebuddy, buddy, smoke, cigarette, rolling,...   \n",
      "7  [nerdwax, slipping, shark, seen, stop, tank, a...   \n",
      "8  [sq, stretchtite, 3000, ft, detailing, slide, ...   \n",
      "9  [beard, application, horsehair, handlebar, bru...   \n",
      "\n",
      "                                 Representative_Docs  \n",
      "0  [Foot Scrubber Callus Remover for Feet : Pedic...  \n",
      "1  [Caretras Posture Corrector for Women and Men,...  \n",
      "2  [𝗣𝗥𝗢𝗙𝗘𝗦𝗦𝗜𝗢𝗡𝗔𝗟 Digital Hanging Scale 660 LB 300...  \n",
      "3  [DownBeats - Ear Plugs for Noise Reduction at ...  \n",
      "4  [Panasonic 30212 Lithium 3V Photo Lithium Batt...  \n",
      "5  [Pressure Positive Co. The Backnobber II (Char...  \n",
      "6  [smokebuddy Smoke Buddy, smokebuddy Smoke Budd...  \n",
      "7  [Nerdwax Stop Slipping Glasses as Seen on Shar...  \n",
      "8  [Stretch-tite 3000 sq. ft, Stretch-tite 3000 s...  \n",
      "9  [Can You Handlebar Mens Beard Balm Application...  \n"
     ]
    }
   ],
   "source": [
    "topic_info = topic_model.get_topic_info()\n",
    "print(topic_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d2651da3-f961-4fb8-af6f-70832647af0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha=0.01, passes=10, num_topics=10, Coherence Score = 0.4157\n",
      "alpha=0.01, passes=10, num_topics=11, Coherence Score = 0.4587\n",
      "alpha=0.01, passes=10, num_topics=12, Coherence Score = 0.4242\n",
      "alpha=0.01, passes=10, num_topics=13, Coherence Score = 0.4481\n",
      "alpha=0.01, passes=10, num_topics=14, Coherence Score = 0.4432\n",
      "alpha=0.01, passes=10, num_topics=15, Coherence Score = 0.3727\n",
      "alpha=0.01, passes=10, num_topics=16, Coherence Score = 0.4084\n",
      "alpha=0.01, passes=10, num_topics=17, Coherence Score = 0.3837\n",
      "alpha=0.01, passes=10, num_topics=18, Coherence Score = 0.4288\n",
      "alpha=0.01, passes=10, num_topics=19, Coherence Score = 0.4381\n",
      "alpha=0.01, passes=10, num_topics=20, Coherence Score = 0.4001\n",
      "alpha=0.01, passes=10, num_topics=21, Coherence Score = 0.4196\n",
      "alpha=0.01, passes=10, num_topics=22, Coherence Score = 0.4283\n",
      "alpha=0.01, passes=10, num_topics=23, Coherence Score = 0.4087\n",
      "alpha=0.01, passes=10, num_topics=24, Coherence Score = 0.4315\n",
      "alpha=0.01, passes=10, num_topics=25, Coherence Score = 0.4253\n",
      "alpha=0.01, passes=10, num_topics=26, Coherence Score = 0.4158\n",
      "alpha=0.01, passes=10, num_topics=27, Coherence Score = 0.4257\n",
      "alpha=0.01, passes=10, num_topics=28, Coherence Score = 0.4269\n",
      "alpha=0.01, passes=10, num_topics=29, Coherence Score = 0.4320\n",
      "alpha=0.01, passes=10, num_topics=30, Coherence Score = 0.4384\n",
      "alpha=0.01, passes=20, num_topics=10, Coherence Score = 0.4124\n",
      "alpha=0.01, passes=20, num_topics=11, Coherence Score = 0.4575\n",
      "alpha=0.01, passes=20, num_topics=12, Coherence Score = 0.4233\n",
      "alpha=0.01, passes=20, num_topics=13, Coherence Score = 0.4399\n",
      "alpha=0.01, passes=20, num_topics=14, Coherence Score = 0.4342\n",
      "alpha=0.01, passes=20, num_topics=15, Coherence Score = 0.3735\n",
      "alpha=0.01, passes=20, num_topics=16, Coherence Score = 0.4142\n",
      "alpha=0.01, passes=20, num_topics=17, Coherence Score = 0.3842\n",
      "alpha=0.01, passes=20, num_topics=18, Coherence Score = 0.4239\n",
      "alpha=0.01, passes=20, num_topics=19, Coherence Score = 0.4281\n",
      "alpha=0.01, passes=20, num_topics=20, Coherence Score = 0.4032\n",
      "alpha=0.01, passes=20, num_topics=21, Coherence Score = 0.4183\n",
      "alpha=0.01, passes=20, num_topics=22, Coherence Score = 0.4193\n",
      "alpha=0.01, passes=20, num_topics=23, Coherence Score = 0.4040\n",
      "alpha=0.01, passes=20, num_topics=24, Coherence Score = 0.4370\n",
      "alpha=0.01, passes=20, num_topics=25, Coherence Score = 0.4198\n",
      "alpha=0.01, passes=20, num_topics=26, Coherence Score = 0.4156\n",
      "alpha=0.01, passes=20, num_topics=27, Coherence Score = 0.4182\n",
      "alpha=0.01, passes=20, num_topics=28, Coherence Score = 0.4331\n",
      "alpha=0.01, passes=20, num_topics=29, Coherence Score = 0.4267\n",
      "alpha=0.01, passes=20, num_topics=30, Coherence Score = 0.4439\n",
      "alpha=0.01, passes=30, num_topics=10, Coherence Score = 0.4074\n",
      "alpha=0.01, passes=30, num_topics=11, Coherence Score = 0.4575\n",
      "alpha=0.01, passes=30, num_topics=12, Coherence Score = 0.4144\n",
      "alpha=0.01, passes=30, num_topics=13, Coherence Score = 0.4375\n",
      "alpha=0.01, passes=30, num_topics=14, Coherence Score = 0.4372\n",
      "alpha=0.01, passes=30, num_topics=15, Coherence Score = 0.3729\n",
      "alpha=0.01, passes=30, num_topics=16, Coherence Score = 0.4111\n",
      "alpha=0.01, passes=30, num_topics=17, Coherence Score = 0.3790\n",
      "alpha=0.01, passes=30, num_topics=18, Coherence Score = 0.4240\n",
      "alpha=0.01, passes=30, num_topics=19, Coherence Score = 0.4247\n",
      "alpha=0.01, passes=30, num_topics=20, Coherence Score = 0.3970\n",
      "alpha=0.01, passes=30, num_topics=21, Coherence Score = 0.4152\n",
      "alpha=0.01, passes=30, num_topics=22, Coherence Score = 0.4251\n",
      "alpha=0.01, passes=30, num_topics=23, Coherence Score = 0.4042\n",
      "alpha=0.01, passes=30, num_topics=24, Coherence Score = 0.4374\n",
      "alpha=0.01, passes=30, num_topics=25, Coherence Score = 0.4192\n",
      "alpha=0.01, passes=30, num_topics=26, Coherence Score = 0.4144\n",
      "alpha=0.01, passes=30, num_topics=27, Coherence Score = 0.4169\n",
      "alpha=0.01, passes=30, num_topics=28, Coherence Score = 0.4325\n",
      "alpha=0.01, passes=30, num_topics=29, Coherence Score = 0.4254\n",
      "alpha=0.01, passes=30, num_topics=30, Coherence Score = 0.4434\n",
      "alpha=0.1, passes=10, num_topics=10, Coherence Score = 0.3794\n",
      "alpha=0.1, passes=10, num_topics=11, Coherence Score = 0.4449\n",
      "alpha=0.1, passes=10, num_topics=12, Coherence Score = 0.4192\n",
      "alpha=0.1, passes=10, num_topics=13, Coherence Score = 0.4488\n",
      "alpha=0.1, passes=10, num_topics=14, Coherence Score = 0.4258\n",
      "alpha=0.1, passes=10, num_topics=15, Coherence Score = 0.4212\n",
      "alpha=0.1, passes=10, num_topics=16, Coherence Score = 0.4120\n",
      "alpha=0.1, passes=10, num_topics=17, Coherence Score = 0.4297\n",
      "alpha=0.1, passes=10, num_topics=18, Coherence Score = 0.4288\n",
      "alpha=0.1, passes=10, num_topics=19, Coherence Score = 0.4135\n",
      "alpha=0.1, passes=10, num_topics=20, Coherence Score = 0.4204\n",
      "alpha=0.1, passes=10, num_topics=21, Coherence Score = 0.4301\n",
      "alpha=0.1, passes=10, num_topics=22, Coherence Score = 0.4130\n",
      "alpha=0.1, passes=10, num_topics=23, Coherence Score = 0.4212\n",
      "alpha=0.1, passes=10, num_topics=24, Coherence Score = 0.4440\n",
      "alpha=0.1, passes=10, num_topics=25, Coherence Score = 0.4024\n",
      "alpha=0.1, passes=10, num_topics=26, Coherence Score = 0.4188\n",
      "alpha=0.1, passes=10, num_topics=27, Coherence Score = 0.4322\n",
      "alpha=0.1, passes=10, num_topics=28, Coherence Score = 0.4498\n",
      "alpha=0.1, passes=10, num_topics=29, Coherence Score = 0.4138\n",
      "alpha=0.1, passes=10, num_topics=30, Coherence Score = 0.4373\n",
      "alpha=0.1, passes=20, num_topics=10, Coherence Score = 0.3750\n",
      "alpha=0.1, passes=20, num_topics=11, Coherence Score = 0.4511\n",
      "alpha=0.1, passes=20, num_topics=12, Coherence Score = 0.4180\n",
      "alpha=0.1, passes=20, num_topics=13, Coherence Score = 0.4376\n",
      "alpha=0.1, passes=20, num_topics=14, Coherence Score = 0.4364\n",
      "alpha=0.1, passes=20, num_topics=15, Coherence Score = 0.4307\n",
      "alpha=0.1, passes=20, num_topics=16, Coherence Score = 0.4097\n",
      "alpha=0.1, passes=20, num_topics=17, Coherence Score = 0.4240\n",
      "alpha=0.1, passes=20, num_topics=18, Coherence Score = 0.4259\n",
      "alpha=0.1, passes=20, num_topics=19, Coherence Score = 0.4164\n",
      "alpha=0.1, passes=20, num_topics=20, Coherence Score = 0.4202\n",
      "alpha=0.1, passes=20, num_topics=21, Coherence Score = 0.4321\n",
      "alpha=0.1, passes=20, num_topics=22, Coherence Score = 0.4128\n",
      "alpha=0.1, passes=20, num_topics=23, Coherence Score = 0.4201\n",
      "alpha=0.1, passes=20, num_topics=24, Coherence Score = 0.4403\n",
      "alpha=0.1, passes=20, num_topics=25, Coherence Score = 0.4011\n",
      "alpha=0.1, passes=20, num_topics=26, Coherence Score = 0.4178\n",
      "alpha=0.1, passes=20, num_topics=27, Coherence Score = 0.4339\n",
      "alpha=0.1, passes=20, num_topics=28, Coherence Score = 0.4489\n",
      "alpha=0.1, passes=20, num_topics=29, Coherence Score = 0.4111\n",
      "alpha=0.1, passes=20, num_topics=30, Coherence Score = 0.4349\n",
      "alpha=0.1, passes=30, num_topics=10, Coherence Score = 0.3740\n",
      "alpha=0.1, passes=30, num_topics=11, Coherence Score = 0.4507\n",
      "alpha=0.1, passes=30, num_topics=12, Coherence Score = 0.4266\n",
      "alpha=0.1, passes=30, num_topics=13, Coherence Score = 0.4293\n",
      "alpha=0.1, passes=30, num_topics=14, Coherence Score = 0.4343\n",
      "alpha=0.1, passes=30, num_topics=15, Coherence Score = 0.4227\n",
      "alpha=0.1, passes=30, num_topics=16, Coherence Score = 0.4174\n",
      "alpha=0.1, passes=30, num_topics=17, Coherence Score = 0.4243\n",
      "alpha=0.1, passes=30, num_topics=18, Coherence Score = 0.4252\n",
      "alpha=0.1, passes=30, num_topics=19, Coherence Score = 0.4199\n",
      "alpha=0.1, passes=30, num_topics=20, Coherence Score = 0.4194\n",
      "alpha=0.1, passes=30, num_topics=21, Coherence Score = 0.4271\n",
      "alpha=0.1, passes=30, num_topics=22, Coherence Score = 0.4125\n",
      "alpha=0.1, passes=30, num_topics=23, Coherence Score = 0.4257\n",
      "alpha=0.1, passes=30, num_topics=24, Coherence Score = 0.4415\n",
      "alpha=0.1, passes=30, num_topics=25, Coherence Score = 0.3965\n",
      "alpha=0.1, passes=30, num_topics=26, Coherence Score = 0.4174\n",
      "alpha=0.1, passes=30, num_topics=27, Coherence Score = 0.4329\n",
      "alpha=0.1, passes=30, num_topics=28, Coherence Score = 0.4489\n",
      "alpha=0.1, passes=30, num_topics=29, Coherence Score = 0.4117\n",
      "alpha=0.1, passes=30, num_topics=30, Coherence Score = 0.4337\n",
      "alpha=1.0, passes=10, num_topics=10, Coherence Score = 0.4035\n",
      "alpha=1.0, passes=10, num_topics=11, Coherence Score = 0.4318\n",
      "alpha=1.0, passes=10, num_topics=12, Coherence Score = 0.4422\n",
      "alpha=1.0, passes=10, num_topics=13, Coherence Score = 0.4478\n",
      "alpha=1.0, passes=10, num_topics=14, Coherence Score = 0.4308\n",
      "alpha=1.0, passes=10, num_topics=15, Coherence Score = 0.4417\n",
      "alpha=1.0, passes=10, num_topics=16, Coherence Score = 0.4401\n",
      "alpha=1.0, passes=10, num_topics=17, Coherence Score = 0.4505\n",
      "alpha=1.0, passes=10, num_topics=18, Coherence Score = 0.4491\n",
      "alpha=1.0, passes=10, num_topics=19, Coherence Score = 0.4339\n",
      "alpha=1.0, passes=10, num_topics=20, Coherence Score = 0.4340\n",
      "alpha=1.0, passes=10, num_topics=21, Coherence Score = 0.4548\n",
      "alpha=1.0, passes=10, num_topics=22, Coherence Score = 0.4493\n",
      "alpha=1.0, passes=10, num_topics=23, Coherence Score = 0.4602\n",
      "alpha=1.0, passes=10, num_topics=24, Coherence Score = 0.4587\n",
      "alpha=1.0, passes=10, num_topics=25, Coherence Score = 0.4508\n",
      "alpha=1.0, passes=10, num_topics=26, Coherence Score = 0.4445\n",
      "alpha=1.0, passes=10, num_topics=27, Coherence Score = 0.4487\n",
      "alpha=1.0, passes=10, num_topics=28, Coherence Score = 0.4716\n",
      "alpha=1.0, passes=10, num_topics=29, Coherence Score = 0.4524\n",
      "alpha=1.0, passes=10, num_topics=30, Coherence Score = 0.4477\n",
      "alpha=1.0, passes=20, num_topics=10, Coherence Score = 0.4025\n",
      "alpha=1.0, passes=20, num_topics=11, Coherence Score = 0.4333\n",
      "alpha=1.0, passes=20, num_topics=12, Coherence Score = 0.4327\n",
      "alpha=1.0, passes=20, num_topics=13, Coherence Score = 0.4502\n",
      "alpha=1.0, passes=20, num_topics=14, Coherence Score = 0.4254\n",
      "alpha=1.0, passes=20, num_topics=15, Coherence Score = 0.4337\n",
      "alpha=1.0, passes=20, num_topics=16, Coherence Score = 0.4399\n",
      "alpha=1.0, passes=20, num_topics=17, Coherence Score = 0.4456\n",
      "alpha=1.0, passes=20, num_topics=18, Coherence Score = 0.4506\n",
      "alpha=1.0, passes=20, num_topics=19, Coherence Score = 0.4415\n",
      "alpha=1.0, passes=20, num_topics=20, Coherence Score = 0.4250\n",
      "alpha=1.0, passes=20, num_topics=21, Coherence Score = 0.4439\n",
      "alpha=1.0, passes=20, num_topics=22, Coherence Score = 0.4416\n",
      "alpha=1.0, passes=20, num_topics=23, Coherence Score = 0.4510\n",
      "alpha=1.0, passes=20, num_topics=24, Coherence Score = 0.4554\n",
      "alpha=1.0, passes=20, num_topics=25, Coherence Score = 0.4344\n",
      "alpha=1.0, passes=20, num_topics=26, Coherence Score = 0.4439\n",
      "alpha=1.0, passes=20, num_topics=27, Coherence Score = 0.4456\n",
      "alpha=1.0, passes=20, num_topics=28, Coherence Score = 0.4661\n",
      "alpha=1.0, passes=20, num_topics=29, Coherence Score = 0.4554\n",
      "alpha=1.0, passes=20, num_topics=30, Coherence Score = 0.4472\n",
      "alpha=1.0, passes=30, num_topics=10, Coherence Score = 0.3922\n",
      "alpha=1.0, passes=30, num_topics=11, Coherence Score = 0.4215\n",
      "alpha=1.0, passes=30, num_topics=12, Coherence Score = 0.4330\n",
      "alpha=1.0, passes=30, num_topics=13, Coherence Score = 0.4490\n",
      "alpha=1.0, passes=30, num_topics=14, Coherence Score = 0.4284\n",
      "alpha=1.0, passes=30, num_topics=15, Coherence Score = 0.4369\n",
      "alpha=1.0, passes=30, num_topics=16, Coherence Score = 0.4427\n",
      "alpha=1.0, passes=30, num_topics=17, Coherence Score = 0.4489\n",
      "alpha=1.0, passes=30, num_topics=18, Coherence Score = 0.4528\n",
      "alpha=1.0, passes=30, num_topics=19, Coherence Score = 0.4458\n",
      "alpha=1.0, passes=30, num_topics=20, Coherence Score = 0.4271\n",
      "alpha=1.0, passes=30, num_topics=21, Coherence Score = 0.4429\n",
      "alpha=1.0, passes=30, num_topics=22, Coherence Score = 0.4426\n",
      "alpha=1.0, passes=30, num_topics=23, Coherence Score = 0.4467\n",
      "alpha=1.0, passes=30, num_topics=24, Coherence Score = 0.4503\n",
      "alpha=1.0, passes=30, num_topics=25, Coherence Score = 0.4282\n",
      "alpha=1.0, passes=30, num_topics=26, Coherence Score = 0.4465\n",
      "alpha=1.0, passes=30, num_topics=27, Coherence Score = 0.4462\n",
      "alpha=1.0, passes=30, num_topics=28, Coherence Score = 0.4633\n",
      "alpha=1.0, passes=30, num_topics=29, Coherence Score = 0.4558\n",
      "alpha=1.0, passes=30, num_topics=30, Coherence Score = 0.4502\n",
      "\n",
      "Best result:\n",
      "alpha=1.0, passes=10, num_topics=28, Coherence = 0.4716\n"
     ]
    }
   ],
   "source": [
    "#LDA Hyper-Tunning\n",
    "\"\"\"\n",
    "#DISPLAY ONLY, DON'T RUN\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, start, limit, step, alpha_vals, passes_vals):\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for alpha in alpha_vals:\n",
    "        for passes in passes_vals:\n",
    "            for num_topics in range(start, limit + 1, step):\n",
    "                model = LdaModel(\n",
    "                    corpus=corpus,\n",
    "                    id2word=dictionary,\n",
    "                    num_topics=num_topics,\n",
    "                    random_state=42,\n",
    "                    iterations=400,\n",
    "                    passes=passes,\n",
    "                    alpha=alpha,\n",
    "                    eta='auto'\n",
    "                )\n",
    "                \n",
    "                coherence_model = CoherenceModel(\n",
    "                    model=model, \n",
    "                    texts=texts, \n",
    "                    dictionary=dictionary, \n",
    "                    coherence='c_v'\n",
    "                )\n",
    "                coherence_score = coherence_model.get_coherence()\n",
    "                \n",
    "                result = {\n",
    "                    'num_topics': num_topics,\n",
    "                    'alpha': alpha,\n",
    "                    'passes': passes,\n",
    "                    'coherence': coherence_score,\n",
    "                    'model': model\n",
    "                }\n",
    "                results.append(result)\n",
    "                print(f\"alpha={alpha}, passes={passes}, num_topics={num_topics}, Coherence Score = {coherence_score:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "alpha_values = [0.01, 0.1, 1.0]\n",
    "passes_values = [10, 20, 30]\n",
    "start = 10\n",
    "limit = 30 \n",
    "step = 1\n",
    "\n",
    "\n",
    "results = compute_coherence_values(\n",
    "    dictionary=dictionary,\n",
    "    corpus=bow_corpus,\n",
    "    texts=merged_df[\"tokens\"].tolist(),\n",
    "    start=start, \n",
    "    limit=limit, \n",
    "    step=step,\n",
    "    alpha_vals=alpha_values,\n",
    "    passes_vals=passes_values\n",
    ")\n",
    "\n",
    "\n",
    "best_result = max(results, key=lambda x: x['coherence'])\n",
    "print(\"\\nBest result:\")\n",
    "print(f\"alpha={best_result['alpha']}, passes={best_result['passes']}, num_topics={best_result['num_topics']}, Coherence = {best_result['coherence']:.4f}\")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
